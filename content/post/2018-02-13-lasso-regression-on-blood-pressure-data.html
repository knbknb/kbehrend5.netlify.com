---
title: Applying Regression Models on imputed Blood Pressure Data
author: Knut Behrends
date: 2018-02-13
slug: bloodpressure-lasso
categories: [health]
tags: [health, medicine]
draft: yes
output:
  html_document:
    toc: no
    css: styles.css
    fig_caption: yes
    keep_md: no
image: /static/img/post/thumb/lasso-1-350.png
banner: img/post/thumb/lasso-1-150.png
summary: ''
keywords:
  - Personal-Blog
  - Fun
  - rstats
  - health
---



<div id="introduction" class="section level4">
<h4>Introduction</h4>
<div class="alert alert-info"><b>Performing model selection techniques  on anonymized data from 10000 people representing the general US Population</b>. I am assuming Blood Pressure is a response variable. There are ~ 70 predictor variables.<br>
This is not a tutorial, but more a personal learning experience, and work in progress.
<div>This blog post deals actually with two subjects: variable imputation and model selection. <br>
Imputation is done incorrectly here, I know that.
Maybe I&apos;ll split it up into two posts later - this post is too long already, anyway.</div>

</div>
<p>This is a follow-up to an <a href="/post/2018/02/bloodpressure-nhanes">exploratory data-analysis post</a> of mine. When I was nearly done with writing that post, some new questions came to my mind.</p>
<p>In the preceding blogpost I have noticed that Systolic Blood Pressure is correlated with age and weight, but the relationship is complicated: the trend was noisy and the correlation was weak, and the interaction term in the linear model seems to play an important role.</p>
<p>Here I’ll continue with exploratory analysis. More basically, can I determine some other variables (out of the 76) that are also influencing the Blood Pressure in some way?</p>
<p>I am not trying to do groundbreaking medical research here. Hey I’m just a blogger, playing with a dataset for personal entertainment, trying out some of those fancy Machine Learning algorithms that I encountered during MOOC homework assignments. This time I’ll try these techniques out on a larger dataset that <em>I</em> find interesting.</p>
</div>
<div id="nhanes-dataset" class="section level4">
<h4>NHANES dataset</h4>
<p>NHANES is the US National Health and Nutrition Examination Study. It is a carefully curated, larger medical survey aiming to get a representative sample of the general US population. The survey is carried out periodically.</p>
<p>The <a href="https://www.cdc.gov/nchs/nhanes/participant_video.htm"><em>NHANES</em> dataset</a> is available as an R package on <a href="https://cran.r-project.org/package=NHANES">CRAN</a>. I have used package version 2.1.0, specifically “NHANES 2009-2012 with adjusted weighting”. This dataset contains data from two collection periods, 2009-2010 and 2011-2012.</p>
<p>Load the preprocessed dataset:</p>
<p>Scale the data frame. This is necessary because the <span class="math inline">\(\lambda\)</span> hyperparameter of ridge regression and lasso regression works best when features have comparable values.</p>
</div>
<div id="model-selection---what-are-the-best-predictor-variables" class="section level2">
<h2>Model Selection - what are the best predictor variables?</h2>
<p>We will do Forward Selection.</p>
<p>For Cross-validation we will use 10 folds.</p>
<pre><code>## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:
## Reordering variables and trying again:</code></pre>
<p><img src="/post/2018-02-13-lasso-regression-on-blood-pressure-data_files/figure-html/regsubsets-1.png" width="864" /></p>
</div>
<div id="ridge-regression-and-the-lasso" class="section level2">
<h2>Ridge Regression and the Lasso</h2>
<p>This section follows the respective chapter from “<a href="http://www-bcf.usc.edu/~gareth/ISL/">Introduction to Statistical Learning</a>”.</p>
<p>The package <code>glmnet</code> does not use the model formula language. So we will set up a model-matrix <code>x</code> of predictors and a response vector <code>y</code>.</p>
<p>A ridge-regression model is created by calling <code>glmnet</code> with <code>alpha = 0</code>, and a lasso regression with <code>alpha = 1</code>. There is also a <code>cv.glmnet</code> function which will do the cross-validation.</p>
<p><img src="/post/2018-02-13-lasso-regression-on-blood-pressure-data_files/figure-html/plotridge-1.png" width="864" /><img src="/post/2018-02-13-lasso-regression-on-blood-pressure-data_files/figure-html/plotridge-2.png" width="864" /></p>
<p>Fit a lasso model with <code>alpha=1</code>.</p>
<p><img src="/post/2018-02-13-lasso-regression-on-blood-pressure-data_files/figure-html/lasso-1.png" width="864" /><img src="/post/2018-02-13-lasso-regression-on-blood-pressure-data_files/figure-html/lasso-2.png" width="864" /></p>
<p>Form cross-validation, the coefficients vector from the best model is:</p>
<pre><code>## 47 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        1
## (Intercept)       0.0699
## Genderfemale     -0.0927
## Gendermale        .     
## Age               0.3812
## Race12            .     
## Race13            .     
## Race14            .     
## Race15            .     
## Education2        .     
## Education3        .     
## Education4        .     
## Education5       -0.0972
## HHIncomeMid       .     
## Poverty          -0.0249
## HomeRooms         .     
## HomeOwn2          .     
## HomeOwn3          .     
## Work2             0.0554
## Work3             .     
## Weight            .     
## Height            .     
## BMI               0.0196
## Pulse             .     
## BPDiaAve          0.3114
## DirectChol        .     
## TotChol           .     
## UrineVol1         .     
## UrineFlow1        .     
## Diabetes2         0.0334
## HealthGen2       -0.0547
## HealthGen3        .     
## HealthGen4        .     
## HealthGen5        .     
## DaysPhysHlthBad   .     
## DaysMentHlthBad   .     
## LittleInterest2   .     
## LittleInterest3   .     
## Depressed2        .     
## Depressed3        .     
## SleepHrsNight     .     
## SleepTrouble2     .     
## PhysActive2       .     
## Alcohol12PlusYr2  .     
## Smoke1002         .     
## Smoke100nSmoker   .     
## PregnantNow2      .     
## PregnantNow3      .</code></pre>
<p>Perform a train/validation split at 80% of observations, and find the best <code>lambda</code> for the lasso.</p>
<pre class="r"><code>ntrain   &lt;- as.integer(nrow(NHANES_imputed) * 0.8)
train    &lt;- sample(seq(nrow(NHANES_imputed)), ntrain, replace = FALSE)
lasso.tr &lt;- glmnet(x[train, ], y[train])
#
pred_test     &lt;- predict(lasso.tr, x[-train, ])

# this recycles the vector many times! (R&#39;s implicit-wrap-around feature):
rmse     &lt;- sqrt(apply((y[-train] - pred_test) ^ 2, 2, mean))
#plot(log(lasso.tr$lambda), rmse, type = &quot;b&quot;, xlab = &quot;Log(lambda)&quot;)
lassos_df &lt;- data.frame(log_lass=log(lasso.tr$lambda), rmse=rmse)
# extract the best lambda: sort asc, pick first
lam.best &lt;- lasso.tr$lambda[order(rmse)[1]]

ggplot(lassos_df, aes(log_lass, rmse)) + 
        geom_point() +
        xlab(&quot;Log(lambda)&quot;) +
        ggtitle(&quot;Lasso Regression&quot;, 
                sprintf(&quot;%s\nBest lambda: %0.8f&quot;, 
                        &quot;RMSE of training set&quot;, lam.best)) +
        theme(legend.position=&quot;none&quot;)</code></pre>
<p><img src="/post/2018-02-13-lasso-regression-on-blood-pressure-data_files/figure-html/predlasso-1.png" width="864" /></p>
<div id="most-important-parameters" class="section level4">
<h4>Most important parameters</h4>
<p>So which of the imputed parameters <em>are</em> most important?</p>
<pre><code>##            rowname    coef
## 1              Age  0.3903
## 2         BPDiaAve  0.3463
## 3      (Intercept)  0.2631
## 4        Diabetes2  0.1572
## 5            Work2  0.1190
## 6      PhysActive2  0.0590
## 7       HealthGen3  0.0525
## 8           Height -0.0582
## 9    SleepTrouble2 -0.0604
## 10      Education2 -0.0619
## 11      Depressed3 -0.0655
## 12      HealthGen2 -0.0837
## 13         Poverty -0.0868
## 14          Race14 -0.0914
## 15 LittleInterest3 -0.1053
## 16      Education5 -0.1654
## 17          Race13 -0.1658
## 18          Race12 -0.1798
## 19          Race15 -0.1883
## 20    Genderfemale -0.2424</code></pre>
</div>
<div id="conclusions" class="section level4">
<h4>Conclusions</h4>
<p>There are no conclusions. The imputation procedure has created too much misleading information. (TBC)</p>
<div class="alert alert-info"><b>What I&apos; learned from doing this:</b>
<ul>
<li>Practiced imputation of a dataset with many NA values present.
<li>Applied Model Selection techniques, a little bit more independently than before 
<li>Learned when best-subset-selection is infeasible (code not shown) 
  
</ul>
</div>
<p>Obligatory Disclaimer (from the NHANES Package Documentation):</p>
<p><em>Please note that the data sets provided in this package are derived from the NHANES database and have been adapted for educational purposes. As such, they are NOT suitable for use as a research database.</em></p>
</div>
</div>

---
title: Applying Regression Models on imputed Blood Pressure data
author: Knut Behrends
date: '2018-02-13'
slug: bloodpressure-lasso
categories: [health]
tags: [health, medicine]
draft: yes
output:
  html_document:
    toc: no
    css: styles.css
    fig_caption: yes
    keep_md: no
image: /static/img/hugo-gopher.png
banner: img/post/thumb/hugo-logo.png
summary: ''
keywords:
  - Personal-Blog
  - Fun
  - rstats
  - health
---


```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
               echo = FALSE, tidy = FALSE, width = 7, fig.width=9)
options(digits = 3, tibble.print_min = 6)
options(knitr.table.format = "html") 
library(kableExtra)
```

```{r pkgs1}
library(mice)  # imputation

library(NHANES) # Body Shape + related measurements from 10000 US citizens
data(NHANES)

library(rvest)
library(gridExtra)
library(tidyverse)
library(ggthemes)
#library(caTools) #sample.split()
library(leaps) # forward selection
theme_set(theme_bw())
scale_colour_discrete <- function(...) scale_color_brewer(palette="Set1")

```

####  Introduction 


```{r abstr}

htmltools::HTML('<div class="alert alert-info"><b>Performing model selection techniques  on anonymized data from 10000 people representing the general US Population</b>. I am assuming Blood Pressure is a response variable. There are ~ 70 predictor variables.<br>
This is not a tutorial, but more a personal learning experience, and work in progress.
</div>')

```

This is a follow-up to [an exploratory data-analysis post](/post/2018/02/bloodpressure-nhanes) of mine. When I was nearly done with writing that post, some new questions came to my mind.

In the preceding blogpost I noticed that Systolic Blood Pressure is correlated with age and weight, but the relationship is complicated: the trend was noisy and the correlation was weak, and interaction term in the linear model seems to play an important role. 

Here I'll continue with exploratory analysis, so there is not really a new hypothesis. More basically, can I determine some other variables (out of the `r ncol(NHANES)`) that are also influencing the Blood Pressure in some way?

I am not trying to do groundbreaking medical research here. Hey I'm just a blogger, playing with a dataset for personal entertainment, trying out some of those fancy Machine Learning algorithms that I encountered during MOOC homework assignments. This time I'll try these techniques out on larger datasets that *I* find interesting.


#### NHANES dataset

NHANES is  the US National Health and Nutrition Examination Study. It is a carefully curated, larger medical survey aiming to get a representative sample of the general US population. The survey is carried out periodically.

The [*NHANES* dataset](https://www.cdc.gov/nchs/nhanes/participant_video.htm) is available as an R package on [CRAN](https://cran.r-project.org/package=NHANES). I have used package version 2.1.0, specifically "NHANES 2009-2012 with adjusted weighting". This dataset contains data from two collection periods, 2009-2010 and 2011-2012.

Obligatory Disclaimer (from the Package Documentation):

*Please note that the data sets provided in this package are derived from the NHANES database and have been adapted for educational purposes. As such, they are NOT suitable for use as a research database.*

#### Feature Engineering
These are  the column names and some other important metadata of the NHANES dataset. These metadata are, for example, the unique distinct values of columns where it makes sense to report them (e.g. male/female for column "gender") 


```{r names_defs}
x <- read_html(x = here::here("data_private/nhanes_datadict_raw.html"))
dls <- html_nodes(x, xpath="//dl") 
nhanes_defs = data.frame()
dl <- dls[[1]]
dts <- html_nodes(dl, xpath="//dt")  
dds <- html_nodes(dl, xpath="//dd")
nhanes_defs <- data_frame(
                  "Column" = dts %>% 
                           html_nodes(xpath='//dt')%>% html_text(),
                   "Meaning" = dds %>% html_nodes(xpath='//dd')%>% 
                           html_text() %>% str_trim() %>% 
                           str_replace_all("\t", " ") %>%  
                           str_replace("\"","")
                   )
```

```{r drop}

action1 <- "drop"
action2 <- "Yes/No"

nhanes_defs <- nhanes_defs %>%
        mutate(Action= case_when(
# (str_detect(Meaning, "Yes") == TRUE)  & 
#  str_detect(Meaning, "No") == TRUE ~ action2, 
         Column == "ID" ~ action1,
         Column == "AgeMonths" ~ action1,
         Column == "AgeDecade" ~ action1,
         Column == "Race3" ~ action1,
         Column == "Education" ~ action1,
         Column == "MaritalStatus" ~ action1,
         Column == "Length" ~ action1,
         Column == "HeadCirc" ~ action1,
         Column == "MaritalStatus" ~ action1,
         Column == "BMICatUnder20yrs" ~ action1,
         Column == "BMI_WHO" ~ action1,
         str_detect(Column, pattern=regex("^BP")) == TRUE &
         str_detect(Column, pattern=regex("Ave")) == FALSE ~ action1,
         # Column == "nPregnancies" ~ action1,
         # Column == "nBabies" ~ action1,
         # Column == "PregnantNow" ~ action1,
         # Column == "Age1stBaby" ~ action1,
         str_detect(Column, "WTINT2YR") == TRUE ~ action1,
         Column == "TVHrsDayChild" ~ action1,
         Column == "CompHrsDayChild" ~ action1,
#         str_detect(Column, pattern=regex("TVHrs|CompHrs|AgeDecade")) ==
#                 TRUE ~ action1,
        
         TRUE ~ ""))

```

```{r tabledef, cache=FALSE}

knitr::kable(nhanes_defs, row.names = 1, 
              caption = "Column names in NHANES data set,\
              and columns that will be dropped") %>% 
        kable_styling(bootstrap_options = 
                c("striped", "hover"))


```

```{r dropcols}

drop_cols <- nhanes_defs %>% 
        filter(Action == action1) %>% select(Column) %>%
        as_vector()

# factor_cols <- nhanes_defs %>% 
#         filter(Action == action2) %>% select(Column) %>% 
#         as_vector()
#dfr <- NHANES %>% select(Age, AgeDecade) %>% filter(is.na(AgeDecade))


```

More Feature Engineering Measures:

- Remove people less than 20 years of age
- Remove columns, marked as "drop" in table 1 (measured for child/youth-only, etc).
- Impute missing values with "mice" method (Multiple Imputation by Chain Equation)

Imputation is done with the [MICE method](http://stefvanbuuren.github.io/mice/): "Multiple Imputation by Chained Equations".


```{r NHANES_imputed, echo=TRUE, cache=TRUE}

NHANES <- NHANES %>% 
        filter(Age >= 20) %>% 
        select(setdiff(colnames(NHANES), drop_cols) )

NHimp <- "../../data_private/NHANES-imputed.rds"

if (file.exists(NHimp)){
        NHANES_imputed <- readRDS(NHimp)
} else {
        # set.seed(47)
        # NHANES_imputed <- mice::complete(mice(NHANES))
        saveRDS(NHANES_imputed, file = NHimp)
}
```

The method magically removes all NA values by intelligently guessing. 
It can create nonsensical values though, e.g. pregnant fathers.

Missing categorical values are sometimes NHANES_imputed in a weird way:

```{r pregnant, eval=FALSE}

table(NHANES$Gender, NHANES$PregnantNow, useNA = "always") %>% 
     #   prop.table() %>%
     #   round(digits = 2) %>%         
        kable(caption= "Before Imputation")

table(NHANES_imputed$Gender, NHANES_imputed$PregnantNow, useNA = "always") %>%
      #  prop.table() %>%
      #  round(digits = 2) %>% 
        kable(caption="After imputation")

```

However, NHANES Data after Feature Engineering: no more NAs in any column.

```{r contents}
Hmisc::contents(NHANES_imputed)
```

Scale the data frame. This is necessary because the $lambda$ hyperparameter of ridge regression and lasso regression need to have comparable values.

```{r lambda}

colnames_num = map_lgl(colnames(NHANES_imputed), function(x){
  ! is.factor(NHANES_imputed[,x])      
})

coln <- colnames(NHANES_imputed)[colnames_num]

NHANES_imputed <- NHANES_imputed %>% 
        mutate_at(.funs = funs(scale),
        .vars=coln)

```


Model Selection - what are the best predictor variables?
-----------------------------------
```{r folds}
nfolds <- 10

```

We will do Forward Selection. 

For Cross-validation we will use `r nfolds` folds.

```{r eval=TRUE}

set.seed(11)

nvars <- ncol(NHANES_imputed)
# create a vector of nrows, fill it with an equal number of each 1s, 2s, ... 10s.
folds <- sample(rep(seq_len(nfolds), length = nrow(NHANES_imputed)))
cv.errors <- matrix(NA, nfolds, nvars)

## make  a generic function 
# id is the id of the model
predict.regsubsets <- function(object, newdata, id, ...) {
  form  <- as.formula(object$call[[2]])
  mat   <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  mat[, names(coefi)] %*% coefi
}
ml      <- c("forward", "backward")
method_list = list(NULL, NULL)
names(method_list) <- ml
for (m in ml){
  for (k in seq_len(nfolds)) {
    # leave the k th fold out
    best.fit <- regsubsets(BPSysAve ~ ., 
                         data = NHANES_imputed[folds != k, ], 
                         nvmax = nvars, method = m)
    for (i in seq_len(nvars)) {
      # call predict.subsets via generic call
      pred <- predict(best.fit, NHANES_imputed[folds == k, ], id = i)
      cv.errors[k, i] <- mean((NHANES_imputed$BPSysAve[folds == k] - pred) ^ 2)
    }
  }
  method_list[[m]] <- cv.errors
}



plots_list <- map2(method_list, ml, function(cv.errors, nm, nvars){
        # averaging the column means 
        rmse.cv <- sqrt(apply(cv.errors, 2, mean))
        data.frame(param_no = seq_len(nvars), rmse_cv = rmse.cv) %>% 
                ggplot(aes(param_no, rmse_cv)) +
                geom_line() + ylab("RMSE of cv") + xlab("Parameter Number") +
                ggtitle(sprintf("%s Model Selection", str_to_title( nm)), 
                    subtitle=sprintf("%s folds, Cross Validation Results",  
                    nfolds))  
}, nvars)
marrangeGrob(plots_list, nrow=1, ncol=2, top = "")

```



Ridge Regression and the Lasso
-------------------------------

The package `glmnet` does not use the model formula language. 
So we will set up a model-matrix `x` of predictors and a response vector `y`.

```{r glmnet}

library(glmnet)
x = model.matrix(BPSysAve ~ . - 1, data = NHANES_imputed) 
y = NHANES_imputed$BPSysAve

```

A ridge-regression model is created by calling `glmnet` with `alpha=0`, and a lasso regression with alpha=1. 
There is also a `cv.glmnet` function which will do the cross-validation.

```{r plotridge}
library(ggfortify)
fit.ridge <- glmnet(x, y, alpha = 0)

autoplot(fit.ridge) + 
        xlab("log lambda") + ylab("Value of regularization coefficient") +
        ggtitle("Ridge Regression", "(Legend is under construction)") +
        theme(legend.position="none")


cv.ridge <- cv.glmnet(x, y, alpha = 0)
leg1str <- sprintf("log of best Lambda = %.3f  (%.3f)", 
                   log(cv.ridge$lambda.min), cv.ridge$lambda.min)
leg2str <- sprintf("log of best Lambda + 1 SE = %.3f (%.3f)", 
                   log(cv.ridge$lambda.1se), cv.ridge$lambda.1se)
autoplot(cv.ridge) + xlab("log lambda") + 
        xlab("log lambda") + ylab("CV Error") +
        ggtitle("Ridge Regression: Cross Validation Error", sprintf("%s\n%s", leg1str, leg2str)) +
        theme(legend.position="none")


```

Now we fit a lasso model; for this we use the default `alpha=1`.

```{r lasso}
fit.lasso <- glmnet(x, y)
autoplot(fit.lasso) + 
        xlab("log lambda") + ylab("Value of regularization coefficient") +
        ggtitle("Lasso Regression", "(Legend is under construction)") +
        theme(legend.position="none")

#plot(fit.lasso, xvar = "dev", label = TRUE)
#title("Lasso Regression")

cv.lasso <- cv.glmnet(x, y)
leg1str <- sprintf("log of best Lambda = %.3f  (%.3f)", 
                   log(cv.lasso$lambda.min), cv.lasso$lambda.min)
leg2str <- sprintf("log of best Lambda + 1 SE = %.3f (%.3f)", 
                   log(cv.lasso$lambda.1se), cv.lasso$lambda.1se)

autoplot(cv.lasso) + 
        xlab("log lambda") + ylab("Value of regularization coefficient") +
        ggtitle("Lasso Regression", sprintf("%s\n%s", leg1str, leg2str)) +
        theme(legend.position="none")

# plot(cv.lasso)
# legend("topleft", legend = str_c(leg1str, leg2str, sep="; "), pch = 1)
# title(main = paste(c(leg1str, leg2str)))

 # lambda = 69.01 => 6 coefs
```

Form cross-validation, the coefficients vector from the best model is:

```{r coeflasso}
coef(cv.lasso)
```


Perform a train/validation split at 80% of observations, 
and find the best `lambda` for the lasso.

```{r predlasso, echo=TRUE}

ntrain   <- as.integer(nrow(NHANES_imputed) * 0.8)
train    <- sample(seq(nrow(NHANES_imputed)), ntrain, replace = FALSE)
lasso.tr <- glmnet(x[train, ], y[train])
#
pred_test     <- predict(lasso.tr, x[-train, ])

# this recycles the vector many times! (R's implicit-wrap-around feature):
rmse     <- sqrt(apply((y[-train] - pred_test) ^ 2, 2, mean))
#plot(log(lasso.tr$lambda), rmse, type = "b", xlab = "Log(lambda)")
lassos_df <- data.frame(log_lass=log(lasso.tr$lambda), rmse=rmse)
# extract the best lambda: sort asc, pick first
lam.best <- lasso.tr$lambda[order(rmse)[1]]

ggplot(lassos_df, aes(log_lass, rmse)) + 
        geom_point() +
        xlab("Log(lambda)") +
        ggtitle("Lasso Regression", 
                sprintf("%s\nBest lambda: %s", 
                        "RMSE of training set", lam.best)) +
        theme(legend.position="none")



```

#### Most important parameters 

So which of the imputed parameters *are* most important?


```{r coef_best_lambdas}
coefs <- coef(lasso.tr, s = lam.best) 
coefs %>%  as.matrix() %>% as.data.frame() %>% 
        rename("coef" = `1`) %>%
        rownames_to_column() %>% 
        filter(abs(coef) > 0.05) %>% 
        arrange(desc(coef))
```


```{r htmltools}
htmltools::HTML('<div class="alert alert-info"><b>What I&apos; learned from doing this:</b>
<ul>
<li>Applied Model Selection techniques, a little bit more independently than before 
<li>Played a bit with the `ggvis` package, which generates vector graphics instead of SVGs 
</ul>
</div>')

```
